{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965cc71f-c5e9-4c62-b596-29ed91199872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import UQpy as UQ\n",
    "import scipy.stats as stat\n",
    "from scipy.optimize import newton_krylov\n",
    "from scipy.integrate import dblquad\n",
    "from itertools import combinations, chain, repeat\n",
    "from tqdm import trange\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f602e6a-5b31-4744-bd15-d5015471129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_additive(x):\n",
    "    term1 = 2*x[0]\n",
    "    term2 = 3*x[1]\n",
    "    return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93dbdb8a-4de7-4496-9b0f-b458f7bdcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ishigami(x, a=7, b=0.1):\n",
    "    '''Ishigami test function'''\n",
    "    # check whether the input x is a dataframe\n",
    "    \n",
    "    if not isinstance(x, (pd.core.frame.DataFrame, pd.core.series.Series, np.ndarray, list)):\n",
    "        raise TypeError('`x` must be of type pandas.DataFrame, numpy.ndarray, pd.Series, or list')\n",
    "    \n",
    "    if x.shape[0] > 3:\n",
    "        raise ValueError('`x` must have only three arguments at a time')\n",
    "    \n",
    "    return np.sin(x[0]) + a*(np.sin(x[1])**2) + b*(x[2]**4)*np.sin(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af108795-6f09-40f8-836f-72ebc9d02101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_ranking(factors):\n",
    "    # gather indices for sorting factor\n",
    "    temp = np.argsort(factors)[::-1]\n",
    "    # create an array the same shape and type as temp\n",
    "    ranks = np.empty_like(temp)\n",
    "    # rank factors\n",
    "    ranks[temp] = np.arange(len(factors))\n",
    "\n",
    "    return ranks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49bcea5-5928-4b78-8682-c4076dcea1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def apply_unique(func, df, axis=1, *args, **kwargs):\n",
    "    '''Apply a function to unique rows of a DataFrame\n",
    "    for efficiency.'''\n",
    "\n",
    "    applied_df = df.merge(df.drop_duplicates()\n",
    "                         .assign(**{func.__name__: lambda x: x.apply(func, axis=axis)}), \n",
    "                         how='left')\n",
    "    applied_df.index = df.index\n",
    "    \n",
    "    return applied_df\n",
    "    \n",
    "    \n",
    "def scale(df, bounds, axis=1, *args, **kwargs):\n",
    "    '''scale the sampled matrix\n",
    "    bounds is a dict with ['ub', 'lb'] keys\n",
    "    the values are lists of the upper and lower bounds\n",
    "    of the parameters/variables/factors'''\n",
    "    \n",
    "    # numpy equivalent for math operations\n",
    "    bounds_np = {key:np.array(value) for key,value in bounds.items()}\n",
    "    display(bounds_np)\n",
    "    \n",
    "    if axis:\n",
    "        return df * (bounds_np['ub'] - bounds_np['lb']) + bounds_np['lb']\n",
    "    else:\n",
    "        return df.T * (bounds_np['ub'] - bounds_np['lb']) + bounds_np['lb']\n",
    "    \n",
    "    \n",
    "def pairs_h(iterable):\n",
    "    '''gives the pairs of numbers considering their differences'''\n",
    "    interval = range(min(iterable), max(iterable)-min(iterable))\n",
    "    pairs  = {key+1:[j for j in combinations(iterable, 2) if np.abs(j[0]-j[1])==key+1] for key in interval}\n",
    "    return pairs\n",
    "    \n",
    "    \n",
    "def section_df(df, delta_h): # ***delta_h here is newly added*** July 6th, 2021 - Saman's comment\n",
    "    '''gets the paired values of each section based on index'''\n",
    "    pairs = pairs_h(df.index.get_level_values(-1))\n",
    "    df_values = df.to_numpy()\n",
    "    sample = pd.concat({h*delta_h:\n",
    "                    pd.DataFrame.from_dict({str(idx_tup): [df_values[idx_tup[0]], df_values[idx_tup[1]]] for idx_tup in idx}, 'index') \\\n",
    "                      for h, idx in pairs.items()}) \n",
    "\n",
    "    return sample\n",
    "    \n",
    "    \n",
    "# lambda functions\n",
    "'''covariogram of each section'''\n",
    "cov_section = lambda pair_cols, mu_star: (pair_cols.sub(mu_star, axis=0)[0] * pair_cols.sub(mu_star, axis=0)[1]).groupby(level=[0,1,2]).mean()\n",
    "\n",
    "'''variogram over all sections'''\n",
    "variogram = lambda pair_cols: 0.5*(pair_cols[0] - pair_cols[1]).pow(2).groupby(level=[1,2]).mean()\n",
    "\n",
    "'''morris sensitivity measure equivalent evaluated over all sections'''\n",
    "morris_eq = lambda pair_cols: ((pair_cols[1] - pair_cols[0]).abs().groupby(level=[1,2]).mean(), \\\n",
    "                               (pair_cols[1] - pair_cols[0]).groupby(level=[1,2]).mean())\n",
    "\n",
    "'''covariogram over all sections'''\n",
    "covariogram = lambda pair_cols, mu_overall: ((pair_cols - mu_overall)[0] * (pair_cols - mu_overall)[1]).groupby(level=[1,2]).mean()\n",
    "\n",
    "'''expected covariogram over all sections'''\n",
    "e_covariogram = lambda cov_section_all: cov_section_all.groupby(level=[1,2]).mean()\n",
    "\n",
    "'''sobol (total order) sensitivity measure equivalent evaluated over all sections''' # new sobol added *** 6 July 2021\n",
    "# sobol_eq = lambda gamma, ecov, variance: ((gamma + ecov) / variance).loc[:,1]\n",
    "sobol_eq = lambda gamma, ecov, variance, delta_h: ((gamma + ecov) / variance)[:, delta_h] # new July 6, 2021\n",
    "\n",
    "\n",
    "\n",
    "# ivars function\n",
    "def ivars(variogram_array, scale, delta_h):\n",
    "    '''generate Integrated Variogram Across a Range of Scales (IVARS)\n",
    "    by approximating area using right trapezoids having width of `delta_h`\n",
    "    and hights of variogram values'''\n",
    "    num_h  = len(variogram_value.index.levels[-1].to_list())\n",
    "    x_bench= np.arange(start=0, stop=delta_h*(num_h+1), step=delta_h)\n",
    "    x_int  = np.arange(start=0, stop=(scale*10+1)/10, step=delta_h)\n",
    "\n",
    "    # calculate interpolated values for both x (h) and y (variogram)\n",
    "    if x_int[-1] < scale:\n",
    "        x_int.append(scale)\n",
    "    y_bench= [0] + variogram_array.to_list()\n",
    "\n",
    "    y_int  = np.interp(x=x_int, xp=x_bench, fp=y_bench)\n",
    "    \n",
    "    # for loop for each step size to caluclate the area\n",
    "    ivars = 0\n",
    "    for i in range(len(x_int)-1):\n",
    "        ivars += 0.5*(y_int[i+1] + y_int[i]) * (x_int[i+1] - x_int[i])\n",
    "\n",
    "    return ivars\n",
    "\n",
    "# alias\n",
    "ind = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df4b82f-1faf-4290-9666-c64b2c913e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def apply_unique(func, df, axis=1, *args, **kwargs):\n",
    "    '''Apply a function to unique rows of a DataFrame\n",
    "    for efficiency.'''\n",
    "    \n",
    "    tqdm.pandas(desc=func.__name__ + ' evaluation')\n",
    "\n",
    "    applied_df = df.merge(df.drop_duplicates()\n",
    "                         .assign(**{func.__name__: lambda x: x.progress_apply(func, axis=axis)}), \n",
    "                         how='left')\n",
    "    applied_df.index = df.index\n",
    "    \n",
    "    return applied_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b828b23-50e6-427f-83f9-035d79c7ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_pairs(pair_df, num_stars, parameters, df, delta_h):\n",
    "    # gather the actual 'h' differences between each star point value for every pair\n",
    "    # possibly find a faster way to do this later\n",
    "    dist_list = []\n",
    "    for star_centre in tqdm(range(0, num_stars), desc = 'calculating \\'h\\' values'):\n",
    "        param_num=0\n",
    "        for param in parameters.keys():\n",
    "            pairs = pairs_h(df.loc[star_centre, param][param_num].index.get_level_values(-1))\n",
    "            for ignore, idx in pairs.items():\n",
    "                for idx_tup in idx:\n",
    "                    dist_list.append(np.abs(df.loc[star_centre, param][param_num][idx_tup[0]] - df.loc[star_centre, param][param_num][idx_tup[1]]))\n",
    "                                                                \n",
    "    \n",
    "            param_num = param_num + 1\n",
    "    \n",
    "    # loading bar for binning and reording pairs based on new 'h' values\n",
    "    pairs_pbar = tqdm( desc='binning and reording pairs based on \\'h\\' values', total=2, dynamic_ncols=True)\n",
    "    \n",
    "    # this is to check that the above block is computing the differences in the right pair order\n",
    "    pairs_h(df.loc[0, 'x1'][0].index.get_level_values(-1))\n",
    "    \n",
    "    # drop old distance values\n",
    "    pair_df = pair_df.droplevel('h')\n",
    "    \n",
    "    # add new distances to dataframe\n",
    "    pair_df['h'] = dist_list\n",
    "    \n",
    "    # create bin ranges\n",
    "    num_bins = int(1/delta_h) # the number of bins created by delta h\n",
    "    #bins = np.arange(start=delta_h/2, step=delta_h, stop=1) # create middle bin ranges\n",
    "    bins = np.linspace(start=0, stop=3.14-(-3.14), num=num_bins+1)\n",
    "    display(bins) # delete this\n",
    "    \n",
    "    # create labels for the bin ranges which will be the actual delta h values\n",
    "    labels = np.arange(start=delta_h, step=delta_h, stop=1+delta_h)\n",
    "\n",
    "    # bin pair values according to their distances 'h' for each paramter at each star centre\n",
    "    binned_pairs = []\n",
    "    for star_centre in range(0, num_stars):\n",
    "        for param in parameters.keys():\n",
    "            binned_pairs.append(pd.cut(pair_df.loc[star_centre, param, :]['h'], bins=bins, labels=labels).sort_values())\n",
    "    \n",
    "    # put binned pairs into a panda series\n",
    "    binned_pairs = pd.concat(binned_pairs, ignore_index=False)\n",
    "    \n",
    "    pairs_pbar.update(1)\n",
    "    \n",
    "    # re order pairs values according to the bins\n",
    "    pair_df = pair_df.loc[binned_pairs.index]\n",
    "    \n",
    "    # add in new index h, according to bin ranges\n",
    "    # ex.) h = 0.1 = [0-0.15], h = 0.2 = [0.15-0.25]\n",
    "    h = list(binned_pairs.values)\n",
    "    \n",
    "    # drop actual h values for new rounded ones\n",
    "    pair_df.drop(columns='h')\n",
    "    \n",
    "    pair_df['h'] = h\n",
    "    \n",
    "    # format data frame so that it works properly with variogram analsysis functions\n",
    "    pair_df.set_index('h', append=True, inplace=True)\n",
    "    \n",
    "    pair_df = pair_df.reorder_levels(['centre', 'param', 'h', 'pair_ind'])\n",
    "    \n",
    "    sleep(0.1)\n",
    "    pairs_pbar.update(1)\n",
    "    pairs_pbar.close()\n",
    "    \n",
    "    return pair_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b8067c-ab82-41f8-9156-f24850ef4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rx2rn(distpair_type, param1, param2, rxpair):\n",
    "    \n",
    "    # getting the inverse cdf of distribution 1\n",
    "    if (distpair_type[0] == 'unif'):\n",
    "        mu1 = (param1[1] + param1[0])/2\n",
    "        std1 = (param1[1] - param1[0])/12**0.5\n",
    "        inv_cdf1 = lambda x : param1[0] + (param1[1] - param1[0])*x\n",
    "    elif (distpair_type[0] == 'norm'):\n",
    "        mu1 = param1[0]\n",
    "        std1 = param1[1]\n",
    "        inv_cdf1 = lambda x : stat.norm.ppf(x, mu1, std1)\n",
    "    elif (distpair_type[0] == 'triangle'):\n",
    "        mu1 = (param1[0] + param1[1] + param1[2])/3\n",
    "        std1 = (np.sqrt(param1[0]**2+param1[1]**2+param1[2]**2-param1[0]*param1[1]-param1[0]*param1[2]-param1[1]*param1[2]))/np.sqrt(18)\n",
    "        mid1=(param1[2]-param1[0])/(param1[1]-param1[0])\n",
    "        term11= (param1[1]-param1[0])*(param1[2]-param1[0])\n",
    "        term21= (param1[1]-param1[0])*(param1[1]-param1[2])\n",
    "        inv_cdf1 = lambda x : ((param1[0]+np.sqrt(term11)*np.sqrt(x/1))*((x>=0).astype(int))*((x<mid1).astype(int)) + (param1[1]-np.sqrt(term21)*np.sqrt(1-x))*((x>=mid1).astype(int))*((x<1).astype(int)))\n",
    "    elif (distpair_type[0] == 'lognorm'):\n",
    "        mu1= param1[0]\n",
    "        std1=param1[1]\n",
    "        # compute associated normal\n",
    "        cv=std1/mu1**2\n",
    "        m = np.log(mu1/(np.sqrt(1+cv)))\n",
    "        v = np.sqrt(np.log(1+cv))           \n",
    "        inv_cdf1 = lambda x : stat.lognorm.ppf(x, scale=np.exp(m), s=v, loc=0)\n",
    "    elif (distpair_type[0] == 'expo'):\n",
    "        lamda= param1[0]\n",
    "        mu1=1/lamda\n",
    "        std1=1/(lamda**2)\n",
    "        inv_cdf1 = lambda x : stat.expon.ppf(x, scale=mu1)\n",
    "    elif (distpair_type[0] == 'gev'):\n",
    "        mu=param1[0] #location\n",
    "        sigma=param1[1] #scale\n",
    "        k1=param1[2] #shape\n",
    "        inv_cdf1 = lambda x : stat.genextreme.ppf(x,c=k1,scale=sigma,loc=mu);\n",
    "        [mu1,std1] = stat.genextreme.stats(k1,scale=sigma,loc=mu);\n",
    "        \n",
    "    # getting the inverse cdf of distribution 2\n",
    "    if (distpair_type[1] == 'unif'):\n",
    "        mu2 = (param2[1] + param2[0])/2\n",
    "        std2 = (param2[1] - param2[0])/12**0.5\n",
    "        inv_cdf2 = lambda x : param2[0] + (param2[1] - param2[0])*x\n",
    "    elif (distpair_type[1] == 'norm'):\n",
    "        mu2 = param2[0]\n",
    "        std2 = param2[1]\n",
    "        inv_cdf2 = lambda x : stat.norm.ppf(x, mu2, std2)\n",
    "    elif (distpair_type[1] == 'triangle'):\n",
    "        mu2 = (param2[0] + param2[1] + param2[2])/3\n",
    "        std2 = (np.sqrt(param2[0]**2+param2[1]**2+param2[2]**2-param2[0]*param2[1]-param2[0]*param2[2]-param2[1]*param2[2]))/np.sqrt(18)\n",
    "        mid2=(param2[2]-param2[0])/(param2[1]-param2[0])\n",
    "        term12= (param2[1]-param2[0])*(param2[2]-param2[0])\n",
    "        term22= (param2[1]-param2[0])*(param2[1]-param2[2])\n",
    "        inv_cdf2 = lambda x : ((param2[0]+np.sqrt(term12)*np.sqrt(x/1))*((x>=0).astype(int))*((x<mid2).astype(int)) + (param2[1]-np.sqrt(term22)*np.sqrt(1-x))*((x>=mid1).astype(int))*((x<1).astype(int)))\n",
    "    elif (distpair_type[1] == 'lognorm'):\n",
    "        mu2= param2[0]\n",
    "        std2=param2[1]\n",
    "        # compute associated normal\n",
    "        cv=std2/mu2**2\n",
    "        m = np.log(mu2/(np.sqrt(1+cv)))\n",
    "        v = np.sqrt(np.log(1+cv))           \n",
    "        inv_cdf2 = lambda x : stat.lognorm.ppf(x, scale=np.exp(m), s=v, loc=0)\n",
    "    elif (distpair_type[1] == 'expo'):\n",
    "        lamda= param2[0]\n",
    "        mu2=1/lamda\n",
    "        std2=1/(lamda**2)\n",
    "        inv_cdf2 = lambda x : stat.expon.ppf(x, scale=mu2)\n",
    "    elif (distpair_type[1] == 'gev'):\n",
    "        mu=param2[0] #location\n",
    "        sigma=param2[1] #scale\n",
    "        k2=param2[2] #shape\n",
    "        inv_cdf2 = lambda x : stat.genextreme.ppf(x,c=k2,scale=sigma,loc=mu)\n",
    "        [mu2,std2] = stat.genextreme.stats(k2,scale=sigma,loc=mu)\n",
    "    \n",
    "    # bivariate standard normal distribution\n",
    "    stdnorm2_pdf = lambda x1, x2 : np.exp(-1*(x1**2 + x2**2)/2.0)/(2.0*np.pi)\n",
    "    \n",
    "    # integral bound zmax=5.0, zmin = -5.0\n",
    "    integrand = lambda x1, x2 : inv_cdf1(stat.norm.cdf(x1*np.sqrt(1-rxpair**2)+ rxpair*x2,0,1))*inv_cdf2(stat.norm.cdf(x2,0,1))*stdnorm2_pdf(x1, x2)\n",
    "    # compute double integral of integrand with x1 ranging from -5.0 to 5.0 and x2 ranging from -5.0 to 5.0\n",
    "    rn = (dblquad(integrand, -5.0, 5.0, lambda x : -5.0, lambda x : 5.0) - mu1*mu2)/(std1*std2)\n",
    "    \n",
    "    return rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e915ce16-5f60-4286-9cfe-74e0d8f9e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rn2rx(distpair_type, param1, param2, rnpair):    \n",
    "    fun = lambda r : (rnpair - rx2rn(distpair_type, param1, param2, r))\n",
    "    # try to find point x where fun(x) = 0\n",
    "    try:\n",
    "        rx = newton_krylov(fun, rnpair, x_tol=1e-5)\n",
    "    except:\n",
    "        rx = rnpair\n",
    "            \n",
    "    return rx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7130e67-255f-457e-b9c6-7c2d055e0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_2_cornorm(parameters, corr_mat):\n",
    "    # store parameter info in a list\n",
    "    param_info = list(parameters.values())\n",
    "    \n",
    "    corr_n = np.eye(corr_mat.shape[0], corr_mat.shape[1])\n",
    "    for i in range(0, corr_mat.shape[0] - 1):\n",
    "        for j in range(i+1, corr_mat.shape[0]):\n",
    "            # input paramter info (lb, ub, ?, dist type)\n",
    "            corr_n[i][j] = rn2rx([param_info[i][3], param_info[j][3]], [param_info[i][0], param_info[i][1], param_info[i][2]],[param_info[j][0], param_info[j][1], param_info[j][2]],corr_mat[i][j])\n",
    "            # matrix is symmetrical\n",
    "            corr_n[j][i] = corr_n[i][j]\n",
    "    return corr_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862018d4-c471-4888-827b-92a6a6a8bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_2_cornorm_uqpy(parameters, corr_mat):\n",
    "    # store parameter info in a list\n",
    "    param_info = list(parameters.values())\n",
    "    \n",
    "    # list that holds the distributions\n",
    "    dist_object = []\n",
    "    \n",
    "    # put each paramter in its corresponding distribution\n",
    "    for i in range(0, len(parameters)):\n",
    "        if param_info[i][3] == 'unif':\n",
    "            dist_object.append(UQ.Distributions.Uniform(loc=param_info[i][0], scale=param_info[i][1]))\n",
    "        elif param_info[i][3] == 'norm':\n",
    "            dist_object.append(UQ.Distributions.Normal(loc=param_info[i][0], scale=param_info[i][1]))\n",
    "        # UQpy does not have a triangle distribution\n",
    "        #elif param_info[i][3] == 'triangle':\n",
    "            # dist_object.append(UQ.Distributions.)\n",
    "        elif param_info[i][3] == 'lognorm':\n",
    "            dist_object.append(UQ.Distributions.Lognormal(scale=np.exp(param_info[i][0]), s=param_info[i][1], loc=0))\n",
    "        elif param_info[i][3] == 'expo':\n",
    "            dist_object.append(UQ.Distributions.Exponential(loc=param_info[i][0], scale=param_info[i][1]))\n",
    "        elif param_info[i][3] == 'gev':\n",
    "            dist_object.append(UQ.Distributions.GenExtreme(c=param_info[i][2], loc=param_info[i][0], scale=param_info[i][1]))\n",
    "\n",
    "    # do nataf transformation on correlation matrix\n",
    "    nataf_obj = UQ.Transformations.Nataf(dist_object=dist_object, corr_x=corr_mat)\n",
    "\n",
    "    \n",
    "    return nataf_obj.corr_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bad9f51-b1e7-4d41-85b9-482766efa263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n2x_transform(norm_vectors, parameters):\n",
    "    # Transform from correlated standard normal to original distributions\n",
    "    param_info = list(parameters.values())\n",
    "    \n",
    "    # \n",
    "    k = norm_vectors.shape[1]   \n",
    "    x = np.zeros(norm_vectors.shape)\n",
    "\n",
    "    for i in range(0, k):\n",
    "        if param_info[i][3] == 'unif':\n",
    "            lb = param_info[i][0]\n",
    "            ub = param_info[i][1]\n",
    "\n",
    "            x[:, i] = lb + (ub - lb)*stat.norm.cdf(norm_vectors[:, i],0,1)\n",
    "        elif param_info[i][3] == 'norm':\n",
    "            mu = param_info[i][0]\n",
    "            std = param_info[i][1]\n",
    "\n",
    "            x[:, i] = stat.norm.ppf(stat.norm.cdf(norm_vectors[:, i],0,1), mu, std)\n",
    "        elif param_info[i][3] == 'triangle':\n",
    "            a = param_info[i][0]\n",
    "            b = param_info[i][1]\n",
    "            c = param_info[i][2]\n",
    "            mid = (c-a)/(b-a)\n",
    "            term1 = (b-a)*(c-a)\n",
    "            term2 = (b-a)*(b-c)\n",
    "            x_norm = stat.norm.cdf(norm_vector[:, i],0,1)\n",
    "            x[:, i] = (a+np.sqrt(term1)*np.sqrt(x_norm))*((x_norm >= 0).astype(int))*((x_norm < mid).astype(int))+(b-np.sqrt(term2)*np.sqrt((1-x_norm)))*((x_norm >= mid).astype(int))*((x_norm < 1).astype(int))\n",
    "        elif param_info[i][3] == 'lognorm':\n",
    "            mu = param_info[i][0]\n",
    "            std = param_info[i][1]\n",
    "            term1 = std/mu**2\n",
    "            m = np.log(mu/(np.sqrt(1+term1)))\n",
    "            v = np.sqrt(np.log(1+term1))\n",
    "            x[:, i] = np.lognorm.ppf(stat.norm.cdf(norm_vectors[:, i],0,1), scale=np.exp(mu), s=std, loc=0)\n",
    "        elif param_info[i][3] == 'expo':\n",
    "            mu = param_info[i][0]\n",
    "            x[:, i] = np.expon.ppf(stat.norm.cdf(norm_vectors[:, i],0,1), scale=mu)\n",
    "        elif param_info[i][3] == 'gev':\n",
    "            mu = param_info[i][0] # location\n",
    "            sigma = param_info[i][1] # scale\n",
    "            k = param_info[i][2] # shape\n",
    "            x[:, i] = stat.genextreme.ppf(stat.norm.cdf(norm_vectors[:, i],0,1),c=k,scale=sigma,loc=mu)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c69bdf9-a8fa-4ed3-8291-a217af723c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GVARS inputs\n",
    "num_stars = 1000\n",
    "num_dir_samples = 100\n",
    "delta_h = 0.05\n",
    "seed = 123456789\n",
    "ivars_scales = [0.1, 0.3, 0.5]\n",
    "parameters = {'x1' : (5, 10, None, 'unif'),\n",
    "              'x2' : (0, 1, None, 'unif')}\n",
    "corr_mat = np.array([[1, -0.8], [-0.8, 1]])\n",
    "n_var = len(parameters)\n",
    "n_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32b8ddd4-5795-4d80-a3d6-59c8d4c7bd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.81347329],\n",
       "       [-0.81347329,  1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stars_pbar = tqdm(desc='generating star points', total=10, dynamic_ncols=True)\n",
    "cov_mat = map_2_cornorm_uqpy(parameters, corr_mat)\n",
    "#or\n",
    "#cov_mat = map_2_cornorm(parameters, corr_mat)\n",
    "#stars_pbar.update(1)\n",
    "cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece33a3-9933-4526-a4d5-a04cef976a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate independent standard normal samples\n",
    "# the amount of samples is the same as the amount of stars\n",
    "#stars_pbar.update(1)\n",
    "#df3 = pd.read_csv('Book1.csv', sep=',',header=None)\n",
    "#df3.values.shape\n",
    "#U = df3.values\n",
    "#U = np.random.default_rng(seed=seed).multivariate_normal(np.zeros(n_var), np.eye(n_var), size=num_stars)\n",
    "#U\n",
    "#### cholesky method not used\n",
    "#cholU = np.linalg.cholesky(cov_mat)\n",
    "#cholU = cholU.transpose() # to get in correct format for matrix multiplication\n",
    "#Z = np.matmul(U,cholU) # transform samples to standard normal distribution\n",
    "#####\n",
    "\n",
    "#### Note this block can be deleted once we choose to only use numpy instead of cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47dd74-d3c7-49b8-9b98-6a242c4d6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated standard normal samples\n",
    "# the amount of samples is the same as the amount of stars\n",
    "Z = np.random.default_rng(seed=seed).multivariate_normal(np.zeros(n_var), cov=cov_mat, size=num_stars)\n",
    "#stars_pbar.update(1)\n",
    "display(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868e7f9-4b2a-4839-8475-e5d20983e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Nstar actual multivariate samples X\n",
    "X = n2x_transform(Z, parameters)\n",
    "#stars_pbar.update(1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2230473-12d9-44d0-ac79-ae93375fd8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define index matrix of complement subset\n",
    "compsub = np.empty([n_var, n_var-1])\n",
    "for i in range (0, n_var):\n",
    "\n",
    "    temp = np.arange(n_var)\n",
    "    compsub[i] = np.delete(temp, i)   \n",
    "compsub = compsub.astype(int)\n",
    "#stars_pbar.update(1)\n",
    "compsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461e084-e2de-4c2f-bff8-1544104e6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computer coditional variance and conditional expectation for each star center\n",
    "chol_cond_std = []\n",
    "std_cond_norm = []\n",
    "mui_on_noti = np.zeros((len(Z), n_var))\n",
    "for i in range(0, n_var):\n",
    "    noti = compsub[i]\n",
    "    # 2 dimensional or greater matrix case\n",
    "    if (cov_mat[noti,:][:,noti].ndim >= 2):\n",
    "        cond_std = cov_mat[i][i] - np.matmul(cov_mat[i,noti], np.matmul(np.linalg.inv(cov_mat[noti,:][:,noti]),cov_mat[noti,i]))\n",
    "        chol_cond_std.append(np.linalg.cholesky([[cond_std]]).flatten())\n",
    "        std_cond_norm.append(cond_std)\n",
    "        for j in range(0, len(Z)):\n",
    "            mui_on_noti[j][i] = np.matmul(cov_mat[i,noti], np.matmul(np.linalg.inv(cov_mat[noti,:][:,noti]),Z[j,noti]))\n",
    "    # less then 2 dimenional matrix case\n",
    "    else:\n",
    "        cond_std = cov_mat[i][i] - np.matmul(cov_mat[i,noti],np.matmul(cov_mat[noti,:][:,noti],cov_mat[noti,i]))\n",
    "        chol_cond_std.append(np.linalg.cholesky([[cond_std]]).flatten())\n",
    "        std_cond_norm.append(cond_std)\n",
    "        for j in range(0, len(Z)):\n",
    "            mui_on_noti[j][i] = np.matmul(cov_mat[i, noti],np.matmul(cov_mat[noti,:][:,noti]*Z[j, noti]))\n",
    "#stars_pbar.update(1)\n",
    "display(std_cond_norm)\n",
    "display(chol_cond_std)\n",
    "display(mui_on_noti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2d686-e7aa-4cd2-8613-c3eb42f0ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate directional sample:\n",
    "# Create samples in correlated standard normal space\n",
    "all_section_condZ = []\n",
    "condZ = []\n",
    "for j in range(0, num_dir_samples):\n",
    "    stnrm_base = np.random.default_rng().multivariate_normal(np.zeros(n_var), np.eye(n_var), size=num_stars) # is there a way to do this without using cholesky method?\n",
    "    for i in range(0, n_var):\n",
    "        condZ.append(stnrm_base[:, i]*chol_cond_std[i] + mui_on_noti[:, i])\n",
    "    all_section_condZ.append(condZ.copy())\n",
    "    condZ.clear()\n",
    "stars_pbar.update(1)\n",
    "    \n",
    "np.array(all_section_condZ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5138c1-7cca-433d-92b6-a47f775ba042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3 = pd.read_csv('myDataFile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b9338-f325-43d9-915e-cb5189dab584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_section_condZ = df3.values.reshape([25, 3, 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf4b8-9769-46e4-84b9-f1137b5553f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_section_condZ[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82def3cd-1173-4d7b-9b5f-8880f9bf6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to original distribution and compute response surface\n",
    "Xi_on_Xnoti = []\n",
    "tmp1 = []\n",
    "Xi_on_Xnoti_and_Xnoti_temp = []\n",
    "Xi_on_Xnoti_and_Xnoti = []\n",
    "for j in range(0, num_dir_samples):\n",
    "    for i in range(0, len(parameters)):\n",
    "        tmp1.append(n2x_transform(np.array([all_section_condZ[j][i]]).transpose(), parameters).flatten())\n",
    "        tmp2 = X.copy()\n",
    "        tmp2[:, i] = tmp1[i]\n",
    "        Xi_on_Xnoti_and_Xnoti_temp.append(tmp2.copy()) \n",
    "    # attatch results from tmp1 onto Xi_on_Xnoti and Xi_on_Xnoti_and_Xnoti\n",
    "    Xi_on_Xnoti.append(tmp1.copy())\n",
    "    tmp1.clear() # clear for next iteration\n",
    "    Xi_on_Xnoti_and_Xnoti.append(Xi_on_Xnoti_and_Xnoti_temp.copy())\n",
    "    Xi_on_Xnoti_and_Xnoti_temp.clear() # clear for next iteration\n",
    "stars_pbar.update(1)\n",
    "np.array(Xi_on_Xnoti_and_Xnoti).shape # check that shape is the same as all_section condZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0e96a-49d4-40b6-8a83-d8059babd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Star points into a dataframe\n",
    "params = [*parameters]\n",
    "star_points = {}\n",
    "points = {}\n",
    "temp = np.zeros([num_dir_samples, len(parameters)])\n",
    "for i in range(0, num_stars):\n",
    "    for j in range(0, len(parameters)):\n",
    "        for k in range(0, num_dir_samples):\n",
    "            temp[k, :] = Xi_on_Xnoti_and_Xnoti[k][j][i]\n",
    "        points[params[j]] = np.copy(temp)\n",
    "    star_points[i] = points.copy()\n",
    "stars_pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ac6b2-fdc6-4ca2-905d-a398471f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_points_df = pd.concat({key: pd.concat({k: pd.DataFrame(d) for k, d in value.items()}) for key, value in star_points.items()})\n",
    "star_points_df.index.names=['centre', 'param', 'points']\n",
    "stars_pbar.update(1)\n",
    "star_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e733e7-03e7-48bd-96ec-954abcc7e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = linear_additive\n",
    "df = apply_unique(model_name, star_points_df, axis=1)\n",
    "df.index.names=['centre', 'param', 'points']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea66ae-1734-4b0b-ba34-f86ece823eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the paired values of each section based on `h`\n",
    "tqdm.pandas(desc='building pairs')\n",
    "\n",
    "pair_df = df[model_name.__name__].groupby(level=[0,1]).progress_apply(section_df, delta_h=delta_h)\n",
    "pair_df.index.names = ['centre', 'param', 'h', 'pair_ind']\n",
    "pair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06042982-04b1-47d9-8613-acc67393e003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bin and reorder pairs based on 'h'\n",
    "pair_df = reorder_pairs(pair_df, num_stars, parameters, df, delta_h)\n",
    "pair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ecfd6-51c4-4e81-9b0d-cef74af0d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_pbar = tqdm( desc='VARS Analysis', total=10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f596d-56ac-4070-b726-4f2c59e77f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_star calculation\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Averages of model runs (`mu_star`) calculated - access via .mu_star_df')\n",
    "\n",
    "\n",
    "mu_star_df = df[model_name.__name__].groupby(level=[0,1]).mean()\n",
    "mu_star_df.index.names = ['centre', 'param']\n",
    "mu_star_df.unstack(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7145693-4f65-41ef-aa57-e55fb235bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall mu (mean) of the unique evaluated function values over all stars points\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Overall expected value (`mu_overall`) calculated - access via .mu_overall')\n",
    "\n",
    "mu_overall = df[model_name.__name__].unique().mean()\n",
    "mu_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fde585-299d-4345-8d45-09e23d0d0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall var (variance) of the unique evaluated function values over all stars points\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Overall variance (`var_overall`) calculated - access via .var_overall')\n",
    "\n",
    "\n",
    "var_overall = df[model_name.__name__].unique().var(ddof=1)\n",
    "var_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd634c-bcc2-425a-8feb-e9f4e273e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sectional covariogram calculation - content matches MATLAB code style!!\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Sectional covariogram `cov_section_all` calculated - access via .cov_section_all')\n",
    "\n",
    "\n",
    "cov_section_all = cov_section(pair_df, mu_star_df)\n",
    "cov_section_all.unstack(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e59a2-d402-4aaf-9b41-543fc44572b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variogram calculation\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Variogram (`gamma`) calculated - access via .gamma')\n",
    "\n",
    "variogram_value = variogram(pair_df)\n",
    "# replace missing values with 0\n",
    "variogram_value = variogram_value.unstack(0).fillna(0).unstack(0)\n",
    "variogram_value.unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a3034-719d-46e0-b76d-383e934b778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# morris calculation\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Morris MAEE and MEE (`maee` and `mee`) calculated - access via .maee and .mee')\n",
    "\n",
    "\n",
    "morris_values = morris_eq(pair_df)\n",
    "morris_values[0].unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624cf06-ab1e-4285-bdb2-50ee9445fa1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "morris_values[1].unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe8c39-feb9-41b5-87df-966b02905130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# overall covariogram calculation\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Covariogram (`cov`) calculated - access via .cov')\n",
    "\n",
    "covariogram_value = covariogram(pair_df, mu_overall)\n",
    "covariogram_value.unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422865f-17f6-49af-87fc-f1aca1bca3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected value of the overall covariogram calculation\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Expected value of covariogram (`ecov`) calculated - access via .ecov')\n",
    "\n",
    "e_covariogram_value = e_covariogram(cov_section_all)\n",
    "e_covariogram_value.unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46126795-7ff8-4e35-ab04-d2c614f438d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sobol calculation\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('Sobol ST (`st`) calculated - access via .st')\n",
    "\n",
    "sobol_value = sobol_eq(variogram_value, e_covariogram_value, var_overall, delta_h)\n",
    "sobol_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399ae10-0c1e-4f0d-9fb4-1f4ea8562bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IVARS calculation\n",
    "\n",
    "vars_pbar.update(1)\n",
    "vars_pbar.write('IVARS (`ivars`) calculated - access via .ivars')\n",
    "\n",
    "ivars_values = [0.1, 0.3, 0.5]\n",
    "ivars_df = pd.DataFrame.from_dict({scale: variogram_value.groupby(level=0).apply(ivars, scale=scale, delta_h=delta_h) \\\n",
    "                      for scale in ivars_values}, 'index')\n",
    "ivars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d2003-930a-49db-bb0b-ae4b916539ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384cb9b0-3b87-416d-b81b-1474710da45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating rankings of sobol and ivars\n",
    "sobol_ranking = factor_ranking(sobol_value)\n",
    "sobol_ranking_df = pd.DataFrame(data=[sobol_ranking], columns=parameters.keys(), index=[''])\n",
    "sobol_ranking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efc5b9-9d0b-44ad-aa97-2ff2b4d1083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivars_ranking = []\n",
    "for scale in ivars_scales:\n",
    "    ivars_ranking.append(factor_ranking(ivars_df.loc[scale]))\n",
    "\n",
    "ivars_ranking_df = pd.DataFrame(data=ivars_ranking, columns=parameters.keys(), index=ivars_scales)\n",
    "ivars_ranking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa8375-b5e6-407b-a493-8cbcf3f89ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping to get CIs\n",
    "bootstrap_size = 100\n",
    "\n",
    "# create result dataframes/series if bootstrapping is chosen to be done\n",
    "result_bs_variogram = pd.DataFrame()\n",
    "result_bs_sobol = pd.DataFrame()\n",
    "result_bs_ivars_df = pd.DataFrame()\n",
    "result_bs_sobol_ranking = pd.DataFrame()\n",
    "result_bs_ivars_ranking = pd.DataFrame()\n",
    "\n",
    "for _ in range(0, bootstrap_size):\n",
    "    ## specify random sequence by sampling with replacement\n",
    "    bootstrap_rand = np.random.choice(list(range(0,10)), size=len(range(0,10)), replace=True).tolist()\n",
    "    bootstrapped_pairdf = pd.concat([pair_df.loc[ind[i, :, :, :], :] for i in bootstrap_rand])\n",
    "    bootstrapped_df     = pd.concat([df.loc[ind[i, :, :], :] for i in bootstrap_rand])\n",
    "    #display(bootstrapped_pairdf)\n",
    "    #display(bootstrap_rand)\n",
    "\n",
    "    ## calculating sectional covariograms\n",
    "    bootstrapped_cov_section_all = pd.concat([cov_section_all.loc[ind[i, :]] for i in bootstrap_rand])\n",
    "    #display('sectional variogram:')\n",
    "    #display(bootstrapped_cov_section_all)\n",
    "    #display(bootstrap_rand)\n",
    "\n",
    "    ## calculating variogram, ecovariogram, variance, mean, Sobol, and IVARS values\n",
    "    bootstrapped_variogram = variogram(bootstrapped_pairdf)\n",
    "    # replace missing values with 0\n",
    "    bootstrapped_variogram = bootstrapped_variogram.unstack(0).fillna(0).unstack(0)\n",
    "    #display('variogram:')\n",
    "    #display(bootstrapped_variogram)\n",
    "\n",
    "    bootstrapped_ecovariogram = e_covariogram(bootstrapped_cov_section_all)\n",
    "    #display('E(covariogram):')\n",
    "    #display(bootstrapped_ecovariogram.unstack(level=0))\n",
    "\n",
    "    bootstrapped_var = bootstrapped_df[model_name.__name__].unique().var(ddof=1)\n",
    "    #display('variance:', bootstrapped_var)\n",
    "\n",
    "    bootstrapped_sobol = sobol_eq(bootstrapped_variogram, bootstrapped_ecovariogram, bootstrapped_var, delta_h)\n",
    "    #display('sobol:', bootstrapped_sobol)\n",
    "    \n",
    "    bootstrapped_sobol_ranking = factor_ranking(bootstrapped_sobol)\n",
    "    bootstrapped_sobol_ranking_df = pd.DataFrame(data=[bootstrapped_sobol_ranking], columns=parameters.keys())\n",
    "\n",
    "    bootstrapped_ivars_df = pd.DataFrame.from_dict({scale: bootstrapped_variogram.groupby(level=0).apply(ivars, scale=scale, delta_h=delta_h) \\\n",
    "                                                    for scale in ivars_scales}, 'index')\n",
    "    \n",
    "    bootstrapped_ivars_ranking = []\n",
    "    for scale in ivars_scales:\n",
    "        bootstrapped_ivars_ranking.append(factor_ranking(ivars_df.loc[scale]))\n",
    "\n",
    "    bootstrapped_ivars_ranking_df = pd.DataFrame(data=ivars_ranking, columns=parameters.keys(), index=ivars_scales)\n",
    "    \n",
    "    #display('ivars:', boostrapped_ivars_df)\n",
    "    \n",
    "    # unstack variogram\n",
    "    bootstrapped_variogram_df = bootstrapped_variogram.unstack(level=0)\n",
    "    \n",
    "    # transpose sobol values for stacking of results\n",
    "    bootstrapped_sobol_df = bootstrapped_sobol.to_frame().transpose()\n",
    "    \n",
    "    # attatch new results to previous results (order does not matter here)\n",
    "    result_bs_variogram = pd.concat([bootstrapped_variogram_df, result_bs_variogram])\n",
    "    result_bs_sobol = pd.concat([bootstrapped_sobol_df, result_bs_sobol])\n",
    "    result_bs_ivars_df = pd.concat([bootstrapped_ivars_df, result_bs_ivars_df])\n",
    "    result_bs_sobol_ranking = pd.concat([bootstrapped_sobol_ranking_df, result_bs_sobol_ranking])\n",
    "    result_bs_ivars_ranking = pd.concat([bootstrapped_ivars_ranking_df, result_bs_ivars_ranking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dcbd26-67c9-4769-82a3-5489b6bc4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate upper and lower confidence interval limits of the ivars values\n",
    "ivars_low = pd.DataFrame()\n",
    "ivars_upp = pd.DataFrame()\n",
    "for scale in ivars_scales:\n",
    "    ivars_low = pd.concat([ivars_low, result_bs_ivars_df.loc[scale].quantile((1-0.9)/2).rename(scale).to_frame()], axis=1)\n",
    "    ivars_upp = pd.concat([ivars_upp, result_bs_ivars_df.loc[scale].quantile(1-((1-0.9)/2)).rename(scale).to_frame()], axis=1)\n",
    "\n",
    "ivars_low = ivars_low.transpose()\n",
    "ivars_upp = ivars_upp.transpose()\n",
    "display(ivars_low)\n",
    "display(ivars_upp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b38d7-34c1-46da-95fa-16fc1972f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variogram_low = pd.DataFrame()\n",
    "variogram_upp = pd.DataFrame()\n",
    "for h in np.unique(result_bs_variogram.index.values).tolist():\n",
    "    variogram_low = pd.concat([variogram_low, result_bs_variogram.loc[h].quantile((1-0.9)/2).rename(h).to_frame()], axis=1)\n",
    "    variogram_upp = pd.concat([variogram_upp, result_bs_variogram.loc[h].quantile(1-((1-0.9)/2)).rename(h).to_frame()], axis=1)\n",
    "    \n",
    "variogram_low = variogram_low.transpose()\n",
    "variogram_upp = variogram_upp.transpose()\n",
    "\n",
    "variogram_low.index.names = ['h']\n",
    "variogram_upp.index.names = ['h']\n",
    "\n",
    "display(variogram_low)\n",
    "display(variogram_upp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa763d-946f-4f0b-af53-08ffcf4e106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sobol_low = result_bs_sobol.quantile((1-0.9)/2).rename('').to_frame().transpose()\n",
    "sobol_upp = result_bs_sobol.quantile(1-((1-0.9)/2)).rename('').to_frame().transpose()\n",
    "                            \n",
    "display(sobol_low)\n",
    "display(sobol_upp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a904b4-75ad-42a1-bb4f-75baf994ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_sobol_results = []\n",
    "for param in parameters.keys():\n",
    "    rel_sobol_results.append(result_bs_sobol_ranking[param].eq(sobol_ranking_df[param][0]).sum()/bootstrap_size)\n",
    "\n",
    "rel_sobol = pd.DataFrame([rel_sobol_results],  columns=parameters.keys(), index=[''])\n",
    "rel_sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dce8e8-a44b-4d67-ac3f-0d5c223a27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small test to see if this works properly\n",
    "df_1 = pd.DataFrame({'x1' : [2], 'x2' : [0], 'x3' : [1]}, index = [0])\n",
    "df2 = pd.DataFrame({'x1' : [1, 0, 2, 2], 'x2' : [0, 0, 0, 0], 'x3' : [0, 3, 4, 1]}, index=[0, 0, 0, 0])\n",
    "df2.eq(df_1)['x3'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d30d7-4494-470f-844c-34d6f07d0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate relibability estimate based on ivars factor rankings\n",
    "rel_ivars_results = []\n",
    "for param in parameters.keys():\n",
    "    rel_ivars_results_scale = []\n",
    "    for scale in ivars_scales:\n",
    "        rel_ivars_results_scale.append(result_bs_ivars_ranking[param].loc[scale].eq(ivars_ranking_df[param].loc[scale]).sum()/bootstrap_size)\n",
    "    rel_ivars_results.append(rel_ivars_results_scale)\n",
    "\n",
    "rel_ivars = pd.DataFrame(rel_ivars_results,  columns=ivars_scales, index=parameters.keys())\n",
    "# tranpose to get data in right format\n",
    "rel_ivars = rel_ivars.transpose()\n",
    "\n",
    "rel_ivars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627713f2-abee-4cef-a7d6-e4a963c0d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stat\n",
    "import scipy.cluster.hierarchy as hchy\n",
    "import scipy.spatial.distance as dist\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c9bdc-c8e2-4193-9880-dfd971ca1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_grouping(sens_idx, num_grp=None):\n",
    "    [m, n] = sens_idx.shape\n",
    "\n",
    "    # make data 1d\n",
    "    R = sens_idx.stack()\n",
    "    # replacing zeros with a constant number due to numerical reasoning\n",
    "    R[R == 0] = np.ones(len(R[R == 0]))\n",
    "\n",
    "    # do a box-cox transformation\n",
    "    [TRANSDAT, LAMBDA] = stat.boxcox(R)\n",
    "    if LAMBDA <= 0.0099:\n",
    "        TRANSDAT = np.log(R)\n",
    "\n",
    "    indices = np.argwhere(np.isinf(TRANSDAT).tolist())\n",
    "    if indices.shape == (2, 1):\n",
    "        TRANSDAT[indices[0], indices[1]] = np.log(R[R > 0])\n",
    "\n",
    "    # reshape data for the linkage calculation\n",
    "    S = np.reshape(TRANSDAT.tolist(), [n, m])\n",
    "    \n",
    "    # Agglomerative hierarchical cluster\n",
    "    Z = hchy.linkage(S, method='ward', metric='euclidean')\n",
    "    \n",
    "    # Optimal group number\n",
    "    Clusters = []\n",
    "    for i in range(2, n+1):\n",
    "        Clusters.append(hchy.fcluster(Z, criterion='maxclust', t=i))\n",
    "    # if user gives the group number preform calculations\n",
    "    if num_grp:\n",
    "        rank_grp = hchy.fcluster(Z, criterion='maxclust', t=num_grp)\n",
    "        optm_num_grp = num_grp\n",
    "        nn = 1\n",
    "        id = len(Z)\n",
    "        while nn != optm_num_grp:\n",
    "            cutoff = Z[id-1][2]\n",
    "            rank_grp = hchy.fcluster(Z, criterion='distance', t=cutoff)\n",
    "            nn = np.amax(rank_grp)\n",
    "            id = id - 1\n",
    "\n",
    "    # if user does not give optimal group number use elbow method\n",
    "    else:\n",
    "        cutoff = elbow_method(Z)\n",
    "        rank_grp = hchy.fcluster(Z, criterion='distance', t=cutoff)\n",
    "        optm_num_grp = max(rank_grp)\n",
    "        \n",
    "    \n",
    "    #*** this part can be edited once we start working on plots\n",
    "    #fig = plt.figure(figsize=(25,10))\n",
    "    #dn = hchy.dendrogram(Z)\n",
    "    #plt.show()\n",
    "    \n",
    "    return optm_num_grp, rank_grp, Clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ca206-4bde-4f3b-9b4b-5c70c7908fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(Z):\n",
    "    Q1 = np.array([1, Z[0][2]])\n",
    "    Q2 = np.array([len(Z), Z[-1][2]])\n",
    "    \n",
    "    d = []\n",
    "    for i in range(0, len(Z) - 2):\n",
    "        P = [i+1, Z[i][2]]\n",
    "        d.append(np.abs(np.linalg.det(np.array([[Q2 - Q1], [P-Q1]])))/np.linalg.norm(Q2-Q1))\n",
    "    id = d.index(max(d))\n",
    "    cutoff = Z[id][2]\n",
    "    \n",
    "    return cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953a8ca-12e4-42c6-a8e5-12f5f837c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping\n",
    "num_grp_ivars50, ivars50_grp_array, ClustersIvars50 = factor_grouping(result_bs_ivars_df.loc[0.5], num_grp=2)\n",
    "num_grp_sobol, sobol_grp_array, ClustersSobol = factor_grouping(result_bs_sobol, num_grp=2)\n",
    "\n",
    "display(num_grp_ivars50)\n",
    "display(num_grp_sobol)\n",
    "\n",
    "display(ivars50_grp_array)\n",
    "display(sobol_grp_array)\n",
    "\n",
    "display(ClustersIvars50)\n",
    "display(ClustersSobol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc6e84-714a-455c-93d7-1e463a58236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability estimates of group\n",
    "cluster_sobol = []\n",
    "cluster_rank_sobol = []\n",
    "for g in range(0, num_grp_sobol):\n",
    "    cluster_sobol.append(np.argwhere(sobol_grp_array==g + 1).flatten())\n",
    "    cluster_rank_sobol.append(sobol_ranking_df.to_numpy().flatten()[cluster_sobol[g]])\n",
    "    cluster_rank_sobol[g] = np.sort(cluster_rank_sobol[g], axis=0)\n",
    "\n",
    "display(cluster_sobol)\n",
    "display(cluster_rank_sobol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0543b2-787c-47b5-ace0-797333feeecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ivars50 = []\n",
    "cluster_rank_ivars50 = []\n",
    "for g in range(0, num_grp_ivars50):\n",
    "    cluster_ivars50.append(np.argwhere(ivars50_grp_array==g + 1).flatten())\n",
    "    cluster_rank_ivars50.append(ivars_ranking_df.loc[0.5].to_numpy()[cluster_ivars50[g]])\n",
    "    cluster_rank_ivars50[g] = np.sort(cluster_rank_ivars50[g], axis=0)\n",
    "\n",
    "display(cluster_ivars50)\n",
    "display(cluster_rank_ivars50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67d933-c72f-4c06-b933-2b5dc86e3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reli_sobol_grp_array = np.zeros(len(parameters.keys()))\n",
    "reli_ivars50_grp_array = np.zeros(len(parameters.keys()))\n",
    "for D in range (0, len(parameters.keys())):\n",
    "    match = [np.argwhere(cluster_sobol[x]==D).flatten() for x in range(0, len(cluster_sobol))]\n",
    "    rank_range_sobol = [(match[x].size != 0) for x in range(0, len(match))]\n",
    "    rank_sobol_benchmark = list(compress(cluster_rank_sobol, rank_range_sobol))\n",
    "    rank_sobol_benchmark = rank_sobol_benchmark[0]\n",
    "    \n",
    "    match = [np.argwhere(cluster_ivars50[x]==D).flatten() for x in range(0, len(cluster_ivars50))]\n",
    "    rank_range_ivars50 = [(match[x].size != 0) for x in range(0, len(match))]\n",
    "    rank_ivars50_benchmark = list(compress(cluster_rank_ivars50, rank_range_ivars50))\n",
    "    rank_ivars50_benchmark = rank_ivars50_benchmark[0]\n",
    "    \n",
    "    # calculate the reliability of paramter number D\n",
    "    reli_sobol = 0\n",
    "    reli_ivars50 = 0\n",
    "    for i in range(0, bootstrap_size):\n",
    "        reli_sobol += len(np.argwhere(result_bs_sobol_ranking.iloc[i, D] == rank_sobol_benchmark))/bootstrap_size\n",
    "        reli_ivars50 += len(np.argwhere(result_bs_ivars_ranking.loc[0.5].iloc[i, D] == rank_ivars50_benchmark))/bootstrap_size\n",
    "        \n",
    "    reli_sobol_grp_array[D] = reli_sobol\n",
    "    reli_ivars50_grp_array[D] = reli_ivars50\n",
    "\n",
    "reli_sobol_grp = pd.DataFrame([reli_sobol_grp_array], columns=parameters.keys(), index=[''])\n",
    "reli_ivars50_grp = pd.DataFrame([reli_ivars50_grp_array], columns=parameters.keys(), index=[0.5])\n",
    "    \n",
    "display(reli_sobol_grp)\n",
    "display(reli_ivars50_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91c52a-9b53-4192-81bc-95a5d75807b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the ordering of the groups to match matlab style\n",
    "numgrp = 2\n",
    "for i in range(0, len(ivars50_grp_array)):\n",
    "    ivars50_grp_array[i] = np.abs(ivars50_grp_array[i] - numgrp) + 1\n",
    "   \n",
    "for i in range(0, len(sobol_grp_array)):\n",
    "    sobol_grp_array[i] = np.abs(sobol_grp_array[i] - numgrp) + 1  \n",
    "    \n",
    "display(ivars50_grp_array)\n",
    "display(sobol_grp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20be4fa-dec8-411b-b4f7-788b0189e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display groups\n",
    "ivars50_grps = pd.DataFrame([ivars50_grp_array], columns=parameters.keys(), index=[0.5])\n",
    "sobol_grps = pd.DataFrame([sobol_grp_array], columns=parameters.keys(), index=[''])\n",
    "\n",
    "display(ivars50_grps)\n",
    "display(sobol_grps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2983b5-0294-4af0-a88c-0369e113bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random int for the default seed\n",
    "np.random.randint(1, 123456790)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6473a9a-3b80-48c8-a4dd-db5e7be91e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ymax = variogram_value.unstack(0).loc[0:0.6].max().max()\n",
    "variogram_value.unstack(0).plot(xlabel='Perturbation Scale, h', ylabel='Variogram, $\\gamma$(h)', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f80d62-c27d-4de6-a013-cf40ece3d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data using mean normalization\n",
    "df1 = morris_values[0].unstack(0).iloc[0]\n",
    "df2 = sobol_value\n",
    "df3 = ivars_df.loc[0.5]\n",
    "\n",
    "normalized_maee = df1/df1.sum()\n",
    "normalized_sobol = df2/df2.sum()\n",
    "normalized_ivars50 = df3/df3.sum()\n",
    "display(normalized_sobol)\n",
    "display(normalized_ivars50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02e955-5ac9-4330-aad3-e3cc9ffe2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if bootstrapping is a thing compute bar chart confidence intervals\n",
    "# not sure if this will be included in code\n",
    "ivars50_err_upp = ivars_upp.loc[0.5]/df3.sum()\n",
    "ivars50_err_low = ivars_low.loc[0.5]/df3.sum()\n",
    "sobol_err_upp = (sobol_upp/df2.to_numpy().sum()).to_numpy().flatten()\n",
    "sobol_err_low = (sobol_low/df2.to_numpy().sum()).to_numpy().flatten()\n",
    "\n",
    "# subtract from normalized values so that error bars work properly\n",
    "ivars50_err_upp = np.abs(ivars50_err_upp - normalized_ivars50)\n",
    "ivars50_err_low = np.abs(ivars50_err_low - normalized_ivars50)\n",
    "sobol_err_upp = np.abs(sobol_err_upp - normalized_sobol)\n",
    "sobol_err_low = np.abs(sobol_err_low - normalized_sobol)\n",
    "\n",
    "ivars50_err = np.array([ivars50_err_low, ivars50_err_upp])\n",
    "sobol_err = np.array([sobol_err_low, sobol_err_upp])\n",
    "\n",
    "display(ivars50_err)\n",
    "display(sobol_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e7825-d317-4a3d-b2f9-ddfa946dfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create barchart\n",
    "x = np.arange(len(parameters.keys()))  # the label locations\n",
    "width = 0.1  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, normalized_maee, width, label='VARS-ABE (Morris)')\n",
    "rects2 = ax.bar(x, normalized_ivars50, width, label='IVARS50', yerr=ivars50_err)\n",
    "rects3 = ax.bar(x + width, normalized_sobol, width, label='VARS-TO (Sobol)', yerr=sobol_err)\n",
    "\n",
    "# Add some text for labels, and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Ratio of Factor Importance')\n",
    "ax.set_xlabel('Factor')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(parameters.keys())\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b70e5-02fa-4e67-9171-d2399ace7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_h = 0.05\n",
    "np.arange(start=delta_h/2, step=delta_h, stop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd37f3-0075-4296-a34b-594fcea38b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = 3.14\n",
    "lower_bound = -3.14\n",
    "num_bins = int(1/delta_h)\n",
    "np.linspace(start=0, stop=(upper_bound-lower_bound), num=num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193f306-96ee-46ce-9ff4-2640a5e0214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(start=0, stop=3.14-(-3.14), num=int(1/delta_h)+1)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f74617-93ef-4f7a-93ea-c964b7c81e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
